# Modified from https://github.com/liuxingbin/dbot

import os
import math
import logging
from functools import partial
import torch
import torch.nn as nn

import timm
from timm.models.vision_transformer import VisionTransformer
from timm.models.registry import register_model


class dBOTRandomVisionTransformer(VisionTransformer):
    """Vision Transformer with support for global average pooling."""

    def __init__(self, global_pool=False, **kwargs):
        super().__init__(**kwargs)

        self.global_pool = global_pool
        if self.global_pool:
            norm_layer = kwargs["norm_layer"]
            embed_dim = kwargs["embed_dim"]
            self.fc_norm = norm_layer(embed_dim)

            del self.norm  # remove the original norm

    def forward_features(self, x):
        B = x.shape[0]
        x = self.patch_embed(x)

        # stole cls_tokens impl from Phil Wang, thanks
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)
        x = x + self.pos_embed
        x = self.pos_drop(x)

        for blk in self.blocks:
            x = blk(x)

        # global pool or cls token
        if self.global_pool:
            x = x[:, 1:, :].mean(dim=1)
            outcome = self.fc_norm(x)
        else:
            x = self.norm(x)
            outcome = x[:, 0]

        return outcome


def _load_dbot_random_teacher(model, checkpoint):
    def interpolate_pos_embed(model, checkpoint_model):
        if "pos_embed" in checkpoint_model:
            pos_embed_checkpoint = checkpoint_model["pos_embed"]
            embedding_size = pos_embed_checkpoint.shape[-1]
            num_patches = model.patch_embed.num_patches
            num_extra_tokens = model.pos_embed.shape[-2] - num_patches
            # height (== width) for the checkpoint position embedding
            orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)
            # height (== width) for the new position embedding
            new_size = int(num_patches**0.5)
            # class_token and dist_token are kept unchanged
            if orig_size != new_size:
                print("Position interpolate from %dx%d to %dx%d" % (orig_size, orig_size, new_size, new_size))
                extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
                # only the position tokens are interpolated
                pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
                pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)
                pos_tokens = torch.nn.functional.interpolate(
                    pos_tokens, size=(new_size, new_size), mode="bicubic", align_corners=False
                )
                pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)
                new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
                checkpoint_model["pos_embed"] = new_pos_embed

    def no_grad_trunc_normal_(tensor, mean, std, a, b):
        # Cut & paste from PyTorch official master until it's in a few official releases - RW
        # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
        def norm_cdf(x):
            # Computes standard normal cumulative distribution function
            return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

        if (mean < a - 2 * std) or (mean > b + 2 * std):
            logging.warning(
                "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                "The distribution of values may be incorrect."
            )

        with torch.no_grad():
            # Values are generated by using a truncated uniform distribution and
            # then using the inverse CDF for the normal distribution.
            # Get upper and lower cdf values
            l = norm_cdf((a - mean) / std)
            u = norm_cdf((b - mean) / std)

            # Uniformly fill tensor with values from [l, u], then translate to
            # [2l-1, 2u-1].
            tensor.uniform_(2 * l - 1, 2 * u - 1)

            # Use inverse cdf transform for normal distribution to get truncated
            # standard normal
            tensor.erfinv_()

            # Transform to proper mean, std
            tensor.mul_(std * math.sqrt(2.0))
            tensor.add_(mean)

            # Clamp to ensure it's in the proper range
            tensor.clamp_(min=a, max=b)
            return tensor

    def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
        # type: (Tensor, float, float, float, float) -> Tensor
        return no_grad_trunc_normal_(tensor, mean, std, a, b)

    checkpoint_model = checkpoint["model"]
    state_dict = model.state_dict()
    for k in ["head.weight", "head.bias"]:
        if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:
            print(f"Removing key {k} from pretrained checkpoint")
            del checkpoint_model[k]

    # interpolate position embedding
    if "pos_embed" in checkpoint_model and checkpoint_model["pos_embed"] is not None and model.pos_embed is not None:
        interpolate_pos_embed(model, checkpoint_model)

    # load pre-trained model
    non_matching_keys = model.load_state_dict(checkpoint_model, strict=False)
    logging.info(f"Non-matching keys: {non_matching_keys}")
    print(f"Non-matching keys: {non_matching_keys}")

    # manually initialize fc layer
    trunc_normal_(model.head.weight, std=0.01)

    return model


def _create_dbot_vit(variant: str, pretrained: bool = False, **kwargs) -> dBOTRandomVisionTransformer:
    img_size = int(variant.split("_")[-1])
    pretrained_cfg = {
        "architecture": variant,
        "input_size": (3, img_size, img_size),
        "fixed_input_size": True,
        "interpolation": "bicubic",
        "crop_pct": 0.875,
        "crop_mode": "center",
        "mean": (0.485, 0.456, 0.406),
        "std": (0.229, 0.224, 0.225),
        "num_classes": 1000,
        "pool_size": None,
        "first_conv": "patch_embed.proj",
        "classifier": "head",
    }

    if "pretrained_cfg" not in kwargs or (
        "pretrained_cfg" in kwargs and (kwargs["pretrained_cfg"] is None or kwargs["pretrained_cfg"] == {})
    ):
        kwargs["pretrained_cfg"] = pretrained_cfg
    else:
        raise ValueError("pretrained_cfg already exists and is not empty")

    model = timm.models._builder.build_model_with_cfg(
        dBOTRandomVisionTransformer,
        variant,
        pretrained=False,
        **kwargs,
    )

    if pretrained and variant.startswith("vit_base"):
        # Load checkpoint.
        cache_dir = torch.hub.get_dir()
        file_name = "84.5_dbot_base_pre.pth"
        local_path = os.path.join(cache_dir, "checkpoints", file_name)

        url = f"https://lf3-nlp-opensource.bytetos.com/obj/nlp-opensource/mmodal/dbot/{file_name}"
        if os.path.exists(local_path):
            logging.info(f"Loading {file_name} from local path: {local_path}")
            checkpoint = torch.load(local_path, map_location=torch.device("cpu"))
        else:
            logging.info(f"Downloading {file_name} from url: {url}")
            checkpoint = torch.hub.load_state_dict_from_url(url, map_location=torch.device("cpu"))

        # dBOT modifications.
        model = _load_dbot_random_teacher(model, checkpoint)

    else:
        raise ValueError(f"Pretrained weights not available for {variant}")

    return model


@register_model
def vit_base_patch16_dbot_random_224(pretrained: bool = False, **kwargs) -> dBOTRandomVisionTransformer:
    model_args = dict(
        patch_size=16,
        embed_dim=768,
        depth=12,
        num_heads=12,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
    )
    model = _create_dbot_vit(
        "vit_base_patch16_dbot_random_224",
        pretrained=pretrained,
        **model_args,
        **kwargs,
    )
    return model
